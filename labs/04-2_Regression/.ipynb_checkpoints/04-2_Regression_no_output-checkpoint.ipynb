{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Linear, Polynomial, and Decision Tree Regression</center>\n",
    "<center>Shan-Hung Wu & DataLab</center>\n",
    "<center>Fall 2023</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inline plotting instead of popping out\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will guide you through the linear and polynomial regression using the Housing dataset ([description](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names), [data](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data)). We will also extend the Decision Tree and Random Forest classifiers that we have learned from our previous labs to solve the regression problem.\n",
    "\n",
    "## Linear Regression\n",
    "Regression models are used to predict target variables ($y$'s) in continuous space, which makes them attractive for\n",
    "\n",
    " * Understanding relationships between variables.\n",
    " * Evaluating trends.\n",
    " * Making forecasts.\n",
    " \n",
    "Consider $y$ can be explained by the 1-D points ($x\\in R$), the linear model is defined as:\n",
    "$$\\hat{y} = w_{0}+w_{1}x$$\n",
    "Minimizing the sum of squared errors (SSE) can be understood as finding the best-fitting straight line through the example points. The best-fitting line is called the regression line (or hyperplane when  $x\\in \\mathbb{R}^D$, $D>1$ ), and the vertical offsets from the regression line to the data points are called the **residuals**, i.e. prediction errors, as shown in the following figure:\n",
    "\n",
    "![](./fig-linear-regression.png)\n",
    "\n",
    "Note that the  $w_0$  and  $w_1$  control the intercept / bias and slope of the regression line respectively.\n",
    "## The Housing dataset\n",
    "The Housing dataset from UCI repository collects information about houses in the suburbs of Boston. Following are the attributes:\n",
    "```\n",
    "1.  CRIM      Per capita crime rate by town\n",
    "2.  ZN        Proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3.  INDUS     Proportion of non-retail business acres per town\n",
    "4.  CHAS      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5.  NOX       Nitric oxides concentration (parts per 10 million)\n",
    "6.  RM        Average number of rooms per dwelling\n",
    "7.  AGE       Proportion of owner-occupied units built prior to 1940\n",
    "8.  DIS       Weighted distances to five Boston employment centres\n",
    "9.  RAD       Index of accessibility to radial highways\n",
    "10. TAX       Full-value property-tax rate per \\$10,000\n",
    "11. PTRATIO   Pupil-teacher ratio by town\n",
    "12. B         1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "13. LSTAT     % lower status of the population\n",
    "14. MEDV      Median value of owner-occupied homes in $1000's \n",
    "```\n",
    "\n",
    "Let's load the data first, and see head 5 data in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "    'housing/housing.data',\n",
    "    header=None,\n",
    "    sep='\\s+')\n",
    "\n",
    "df.columns = [\n",
    "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
    "    'PTRATIO', 'B', 'LSTAT', 'MEDV'\n",
    "]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to predict the house prices ('MEDV'), which is in a continuous scale, using the values of some other variable. To select proper explanatory variables, we plot all the pairwise join distributions related to 'MEDV'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vars = [\n",
    "    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
    "    'PTRATIO', 'B', 'LSTAT'\n",
    "]\n",
    "\n",
    "_, subplot_arr = plt.subplots(3, 5, figsize=(20, 12))\n",
    "for idx, x_var in enumerate(x_vars):\n",
    "    x_idx = idx // 5\n",
    "    y_idx = idx % 5\n",
    "    subplot_arr[x_idx, y_idx].scatter(df[x_var], df['MEDV'])\n",
    "    subplot_arr[x_idx, y_idx].set_xlabel(x_var)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this scatter-plot matrix, we can now quickly see how the data is distributed and whether it contains outliers. For example, we can see that there is a linear relationship between RM and the house prices MEDV. Furthermore, we can see in the histogram that both RM and MEDV variable seems to be normally distributed but MEDV contains several **outliers**, i.e. values that deviate from the majority values a lot. Let's use RM as the explanatory variable for our first linear regression task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Linear Regression Model via Scikit-learn  \n",
    "Scikit-learn has already implemented an LinearRegression class that we can make use of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rm = df['RM'].values[:, np.newaxis]\n",
    "y = df['MEDV'].values\n",
    "\n",
    "slr = LinearRegression()\n",
    "# fit \n",
    "slr.fit(X_rm, y)\n",
    "\n",
    "y_pred = slr.predict(X_rm)\n",
    "\n",
    "print('Slope (w_1): %.2f' % slr.coef_[0])\n",
    "print('Intercept/bias (w_0): %.2f' % slr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may interpret the slope 9.10 as the average increase in 'MEDV' due to 'RM'. In contrast, the intercept sometimes also has physical meaning, but not in this case. Since that there is no negative value of a house.  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Next, let's visualize how well the linear regression line fits the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To simplify our codes, predefine a function to visualize to regression line and data scatter plot.\n",
    "def lin_regplot(X, y, model):\n",
    "    plt.scatter(X, y, c='blue')\n",
    "    plt.plot(X, model.predict(X), color='red', linewidth=2)\n",
    "    return\n",
    "\n",
    "\n",
    "lin_regplot(X_rm, y, slr)\n",
    "plt.xlabel('Average number of rooms [RM]')\n",
    "plt.ylabel('Price in $1000\\'s [MEDV]')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the linear regression line reflects the general trend that house prices tend to increase with the number of rooms. Interestingly, we also observe a curious line at  $y=50$ , which suggests that the prices may have been clipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Cases & Performance Evaluation\n",
    "If we have multiple explanatory variables, we can't visualize the linear regression hyperplane in a two-dimensional plot. In this case, we need some other ways to evaluate the trained model. Let's proceed with the multivariate linear regression and evaluate the results using the mean squared error (MSE) and coefficient of determination ($R^2$):\n",
    "   * MSE = $\\dfrac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^2$\n",
    "   * $R^2$ = $1-$ Relative Squared Error(RSE) = $\\dfrac{\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^2}{\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print('# Training data points: %d' % X_train.shape[0])\n",
    "print('# Testing data points: %d' % X_test.shape[0])\n",
    "\n",
    "# Standardization\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# Training\n",
    "slr = LinearRegression()\n",
    "slr.fit(X_train_std, y_train)\n",
    "\n",
    "# Testing\n",
    "y_train_pred = slr.predict(X_train_std)\n",
    "y_test_pred = slr.predict(X_test_std)\n",
    "\n",
    "print('MSE train: %.2f, test: %.2f' %\n",
    "      (mean_squared_error(y_train, y_train_pred),\n",
    "       mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.2f, test: %.2f' % (r2_score(y_train, y_train_pred),\n",
    "                                       r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normal  $R^2$  value should fall between between 0 and 1, and the higher  $R^2$  the better. In practice, we often consider $R^2>0.8$  as good. If $R^2$ is negative, it means that your model doesn't fit your data.\n",
    "\n",
    "NOTE: it is important to standardize the explanatory variables in multivariate regression in order to improve the conditioning of the cost function and to prevent attributes with large values from dominating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Plot\n",
    "In addition, the **residual plot** is a commonly used graphical analysis for a regression model to detect nonlinearity and outliers. In the case of a perfect prediction, the residuals would be exactly zero, which we will probably never encounter in realistic and practical applications. However, for a good regression model, we would expect that the errors are randomly distributed and the residuals should be **randomly scattered around the centerline**. If we see patterns in a residual plot, it means that our model is unable to capture some explanatory information, which is leaked into the residuals (as we can slightly see in the below). Furthermore, we can also use residual plots to detect outliers, which are represented by the points with a large deviation from the centerline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    y_train_pred,\n",
    "    y_train_pred - y_train,\n",
    "    c='blue',\n",
    "    marker='o',\n",
    "    label='Training data')\n",
    "plt.scatter(\n",
    "    y_test_pred,\n",
    "    y_test_pred - y_test,\n",
    "    c='lightgreen',\n",
    "    marker='s',\n",
    "    label='Test data')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='red')\n",
    "plt.xlim([-10, 50])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Linear Regression\n",
    "Now, let's implement our own linear regression model. It is almost the same as the Adaline classifier we have implemented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionGD(object):\n",
    "    def __init__(self, eta=0.001, n_iter=20, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.cost_ = []\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            output = self.net_input(X)\n",
    "            \n",
    "            # Cost function\n",
    "            error = (y - output)\n",
    "            cost = (error ** 2).sum() / 2.0\n",
    "            self.cost_.append(cost)\n",
    "            \n",
    "            # Update rule\n",
    "            self.w_[1:] += self.eta * X.T.dot(error)\n",
    "            self.w_[0] += self.eta * error.sum()\n",
    "            \n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.net_input(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good practice to plot the cost as a function of the number of epochs (passes over the training dataset) when we are using optimization algorithms, such as gradient descent, to check for the convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = StandardScaler()\n",
    "sc_y = StandardScaler()\n",
    "X_rm_std = sc_x.fit_transform(X_rm)\n",
    "y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()\n",
    "\n",
    "lr = LinearRegressionGD()\n",
    "lr.fit(X_rm_std, y_std)\n",
    "\n",
    "plt.plot(range(1, lr.n_iter + 1), lr.cost_)\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('Epoch')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's visualize how well the linear regression line fits the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_regplot(X_rm_std, y_std, lr)\n",
    "plt.xlabel('Average number of rooms [RM] (standardized)')\n",
    "plt.ylabel('Price in $1000\\'s [MEDV] (standardized)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the overall result looks almost identical to the Scikit-learn implementation. Note, however, that the implementation in Scikit-learn makes use of the [LIBLINEAR](http://www.csie.ntu.edu.tw/~cjlin/liblinear/) library and advanced optimization algorithms that work better with unstandardized variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "Linear regression assumes a linear relationship between explanatory and response variables, which may **not** hold in the real world. For example, by seeing the pairwise distribution plot again, we find that the LSTAT (% lower status of the population) attribute is clearly not linearly correlated with our target variable MEDV. Next, let's construct polynomial features and turn our linear regression models into the polynomial ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin = df['LSTAT'].values[:, np.newaxis]\n",
    "\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Create quadratic features\n",
    "quadratic = PolynomialFeatures(degree=2)\n",
    "cubic = PolynomialFeatures(degree=3)\n",
    "X_quad = quadratic.fit_transform(X_lin)\n",
    "X_cubic = cubic.fit_transform(X_lin)\n",
    "\n",
    "# Fit features\n",
    "lin_regr = LinearRegression()\n",
    "lin_regr.fit(X_lin, y)\n",
    "linear_r2 = r2_score(y, lin_regr.predict(X_lin))\n",
    "\n",
    "quad_regr = LinearRegression()\n",
    "quad_regr.fit(X_quad, y)\n",
    "quadratic_r2 = r2_score(y, quad_regr.predict(X_quad))\n",
    "\n",
    "cubic_regr = LinearRegression()\n",
    "cubic_regr.fit(X_cubic, y)\n",
    "cubic_r2 = r2_score(y, cubic_regr.predict(X_cubic))\n",
    "\n",
    "# Plot results\n",
    "X_range = np.arange(X_lin.min(), X_lin.max(), 1)[:, np.newaxis]\n",
    "y_lin_pred = lin_regr.predict(X_range)\n",
    "y_quad_pred = quad_regr.predict(quadratic.fit_transform(X_range))\n",
    "y_cubic_pred = cubic_regr.predict(cubic.fit_transform(X_range))\n",
    "\n",
    "plt.scatter(X_lin, y, label='Training points', color='lightgray')\n",
    "\n",
    "plt.plot(\n",
    "    X_range,\n",
    "    y_lin_pred,\n",
    "    label='Linear (d=1), $R^2=%.2f$' % linear_r2,\n",
    "    color='blue',\n",
    "    lw=2,\n",
    "    linestyle=':')\n",
    "\n",
    "plt.plot(\n",
    "    X_range,\n",
    "    y_quad_pred,\n",
    "    label='Quadratic (d=2), $R^2=%.2f$' % quadratic_r2,\n",
    "    color='red',\n",
    "    lw=2,\n",
    "    linestyle='-')\n",
    "\n",
    "plt.plot(\n",
    "    X_range,\n",
    "    y_cubic_pred,\n",
    "    label='Cubic (d=3), $R^2=%.2f$' % cubic_r2,\n",
    "    color='green',\n",
    "    lw=2,\n",
    "    linestyle='--')\n",
    "\n",
    "plt.xlabel('% lower status of the population [LSTAT]')\n",
    "plt.ylabel('Price in $1000\\'s [MEDV]')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the resulting plot, we can see that the polynomial fit captures the relationship between the response and explanatory variable much better than the linear fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Cases\n",
    "Next, we train polynomial regressors of different degrees using all features in the Housing dataset and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "\n",
    "print('[Linear]')\n",
    "print('# Features: %d' % X_train_std.shape[1])\n",
    "regr = regr.fit(X_train_std, y_train)\n",
    "y_train_pred = regr.predict(X_train_std)\n",
    "y_test_pred = regr.predict(X_test_std)\n",
    "print('MSE train: %.2f, test: %.2f' %\n",
    "      (mean_squared_error(y_train, y_train_pred),\n",
    "       mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.2f, test: %.2f' % (r2_score(y_train, y_train_pred),\n",
    "                                       r2_score(y_test, y_test_pred)))\n",
    "\n",
    "print('\\n[Quadratic]')\n",
    "X_quad_train = quadratic.fit_transform(X_train_std)\n",
    "X_quad_test = quadratic.fit_transform(X_test_std)\n",
    "print('# Features: %d' % X_quad_train.shape[1])\n",
    "regr = regr.fit(X_quad_train, y_train)\n",
    "y_train_pred = regr.predict(X_quad_train)\n",
    "y_test_pred = regr.predict(X_quad_test)\n",
    "print('MSE train: %.2f, test: %.2f' %\n",
    "      (mean_squared_error(y_train, y_train_pred),\n",
    "       mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.2f, test: %.2f' % (r2_score(y_train, y_train_pred),\n",
    "                                       r2_score(y_test, y_test_pred)))\n",
    "\n",
    "print('\\n[Cubic]')\n",
    "X_cubic_train = cubic.fit_transform(X_train_std)\n",
    "X_cubic_test = cubic.fit_transform(X_test_std)\n",
    "print('# Features: %d' % X_cubic_train.shape[1])\n",
    "regr = regr.fit(X_cubic_train, y_train)\n",
    "y_train_pred = regr.predict(X_cubic_train)\n",
    "y_test_pred = regr.predict(X_cubic_test)\n",
    "print('MSE train: %.2f, test: %.2f' %\n",
    "      (mean_squared_error(y_train, y_train_pred),\n",
    "       mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.2f, test: %.2f' % (r2_score(y_train, y_train_pred),\n",
    "                                       r2_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice a very interesting behavior here. As the degree of polynomial goes up, the training errors decrease, but **not** the test errors. That is, **a low training error does not imply a low test error**. We will discuss this further in our next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regression\n",
    "Polynomial regression is not the only way to capture the nonlinear relationship between the explanatory and target variables. For example, we can modify the Decision Tree model for non-linear regression by simply replacing the entropy as the impurity measure of a node by the MSE. Let's see how it works in our task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_3 = DecisionTreeRegressor(max_depth=3)\n",
    "tree_3.fit(X_lin, y)\n",
    "tree_4 = DecisionTreeRegressor(max_depth=4)\n",
    "tree_4.fit(X_lin, y)\n",
    "tree_5 = DecisionTreeRegressor(max_depth=5)\n",
    "tree_5.fit(X_lin, y)\n",
    "\n",
    "sort_idx = X_lin.flatten().argsort()\n",
    "\n",
    "plt.scatter(X_lin, y, color='lightgray')\n",
    "\n",
    "plt.plot(\n",
    "    X_lin[sort_idx],\n",
    "    tree_3.predict(X_lin)[sort_idx],\n",
    "    color='blue',\n",
    "    lw=2,\n",
    "    linestyle=':')\n",
    "plt.plot(\n",
    "    X_lin[sort_idx],\n",
    "    tree_4.predict(X_lin)[sort_idx],\n",
    "    color='red',\n",
    "    lw=2,\n",
    "    linestyle='-')\n",
    "\n",
    "plt.plot(\n",
    "    X_lin[sort_idx],\n",
    "    tree_5.predict(X_lin)[sort_idx],\n",
    "    color='green',\n",
    "    lw=2,\n",
    "    linestyle='--')\n",
    "\n",
    "plt.xlabel('% lower status of the population [LSTAT]')\n",
    "plt.ylabel('Price in $1000\\'s [MEDV]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the resulting plot, the decision tree captures the general trend in the data. However, a limitation of this model is that it does not capture the continuity and differentiability of the desired prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression\n",
    "We can also modify the Random Forest model for regression to take advantages of an ensemble technique and get a better generalization performance. The basic random forests algorithm for regression is almost identical to the random forest algorithm for classification. The only difference is that we use the MSE criterion to grow individual decision trees, and the predicted target variable is calculated as the average prediction over all decision trees. Now, let's use all the features in the Housing dataset to train a random forest regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(\n",
    "    n_estimators=1000, criterion='squared_error', random_state=1, n_jobs=-1)\n",
    "forest.fit(X_train, y_train)\n",
    "y_train_pred = forest.predict(X_train)\n",
    "y_test_pred = forest.predict(X_test)\n",
    "\n",
    "print('MSE train: %.2f, test: %.2f' %\n",
    "      (mean_squared_error(y_train, y_train_pred),\n",
    "       mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2 train: %.2f, test: %.2f' % (r2_score(y_train, y_train_pred),\n",
    "                                       r2_score(y_test, y_test_pred)))\n",
    "\n",
    "# Residual plot\n",
    "plt.scatter(\n",
    "    y_train_pred,\n",
    "    y_train_pred - y_train,\n",
    "    c='blue',\n",
    "    marker='o',\n",
    "    label='Training data')\n",
    "plt.scatter(\n",
    "    y_test_pred, y_test_pred - y_test, c='green', marker='s', label='Test data')\n",
    "\n",
    "plt.xlabel('Predicted values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper left')\n",
    "plt.hlines(y=0, xmin=-10, xmax=50, lw=2, color='red')\n",
    "plt.xlim([-10, 50])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get better testing results ($R^2=0.83$) than those of multivariate linear regression ($R^2=0.67$) and see weaker patterns in the residual plot. However, we still observe that the testing performance is much worse than the training one. Understanding how the testing performance differs from the training performance is crucial, and we will dive into this topic in the next lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: As in the classification, Decision Tree and Random Forest regression has a nice feature that they are **not** sensitive to the scaling of each explanatory variable, thus we do not standardize features this time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "1. Regression models are basically interpolation equations over the range of the explanatory variables. So they may give bad predictions if we extrapolate outside this range.\n",
    "2. Be careful about the outliers, which may change your regression hyperplane undesirably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment</center>\n",
    "In this assignment, you need to train regression models on [Beijing PM2.5 dataset](https://archive.ics.uci.edu/dataset/381/beijing+pm2+5+data) in winter of 2014.\n",
    "1. You have to implement \n",
    "    - a Linear (Polynomial) regressor\n",
    "    - a Random Forest regressor\n",
    "2. You need to show a residual plot for each of your model on both training data and testing data.\n",
    "3. $R^2$ score of **at least one model** need to be larger than 0.72 on testing data.\n",
    "\n",
    "## Requirements:\n",
    "- Submit to **eeclass** with your code file ```Lab04-2_{student_id}.ipynb``` (e.g. ```Lab04-2_110069999.ipynb```).\n",
    "- **The code file should only contain the Assignment part.**\n",
    "- Your .ipynb file should contains the output figures (the residual plots).\n",
    "- Deadline: **2024-01-07 (Sun) 23:59**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Do not modify\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "# inline plotting instead of popping out\n",
    "%matplotlib inline\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'http://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv',\n",
    "    sep=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the latter course, we will teach how to deal with those sample whose has **nan** (not a number) or non-scalar features. For now, we just remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "df = df.drop(['cbwd'], axis=1)  # drop non-scalar feature\n",
    "df = df.dropna(axis=0, how='any')  # drop samples who has nan feature\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we select data that are recorded in winter between 2013 and 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify\n",
    "idx = np.logical_or(\n",
    "    np.logical_and(df['year'].values == 2014, df['month'].values < 3),\n",
    "    np.logical_and(df['year'].values == 2013, df['month'].values == 12))\n",
    "X = df.loc[idx].drop('pm2.5', axis=1)\n",
    "y = df.loc[idx]['pm2.5'].values\n",
    "X.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
