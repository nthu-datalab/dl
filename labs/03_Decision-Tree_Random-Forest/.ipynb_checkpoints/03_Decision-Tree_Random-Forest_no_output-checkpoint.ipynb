{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Decision Trees & Random Forests </center>\n",
    "<center>\n",
    "        Shan-Hung Wu & DataLab\n",
    "        <br>\n",
    "        Fall 2023\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inline plotting instead of popping out\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import load_wine, load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>Decision Trees</b> and <b>Random Forests</b> are two versatile machine learning models that are applicable to many machine learning tasks. In this lab, we will apply them to the classification problem and the dimension reduction problem using the <a href='https://archive.ics.uci.edu/ml/datasets/Wine'>Wine</a> dataset that we have being using in our last lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Machine Learning Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's review the general machine learning steps:\n",
    "1. Data collection, preprocessing (e.g., integration, cleaning, etc.), and exploration.\n",
    "    - Split a dataset into the **training** and **testing** datasets.\n",
    "2. Model development:\n",
    "    - Assume a **model** $\\{f\\}$ that is a collection of candidate functions $f$’s (representing posteriori knowledge) we want to discover. Let's assume that each $f$ is parametrized by $\\boldsymbol{w}$.\n",
    "    - Define a **cost function** $C(\\boldsymbol{w})$  that measures \"how good a particular $f$ can explain the training data.\" The lower the cost function the better.\n",
    "3. **Training**: employ an algorithm that finds the best (or good enough) function $f^∗$  in the model that minimizes the cost function over the training dataset.\n",
    "4. **Testing**: evaluate the performance of the learned $f^*$ using the testing dataset.\n",
    "5. Apply the model in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider a classification task defined over the Wine dataset: to predict the type (class label) of a wine (data point) based on its 13 constituents (attributes / variables / features). Followings are the steps we are going to perform:\n",
    "\n",
    "1. Randomly split the Wine dataset into the training dataset $\\mathbb{X}^{\\text{train}}=\\{(\\boldsymbol{x}^{(i)},y^{(i)})\\}_{i}$ and testing dataset $\\mathbb{X}^{\\text{test}}=\\{(\\boldsymbol{x}'^{(i)},y'^{(i)})\\}_{i}$.\n",
    "2. Model development:\n",
    "    - Model: $\\{f(x)=y\\}$ where each $f$ represents a decision tree.\n",
    "    - Cost function: the <b>entropy</b> (impurity) of class labels of data corresponding to the leaf nodes.\n",
    "3. Training: to grow a tree $f^∗$ by recursively splitting the leaf nodes such that each split leads to the maximal $information$ $gain$ over the corresponding training data.\n",
    "4. Testing: to calculate the <b>prediction accuracy</b> $$\\frac{1}{\\vert\\mathbb{X}^{\\text{test}}\\vert}\\Sigma_{i}1(\\boldsymbol{x}'^{(i)};f^{*}(\\boldsymbol{x}'^{(i)})=y'^{(i)})$$ using the testing dataset.\n",
    "5. Visualize $f^∗$ so we can interpret the meaning of rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data\n",
    "Let's prepare the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine//wine.data', header=None)\n",
    "\n",
    "df.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "              'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "              'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', \n",
    "              'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "X = df.drop(labels='Class label', axis=1)\n",
    "y = df['Class label']\n",
    "#X, y = df.iloc[:, 1:].values, df.iloc[:, 0].values\n",
    "\n",
    "# split X into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print('# Training data points: %d' % X_train.shape[0])\n",
    "print('# Testing data points: %d' % X_test.shape[0])\n",
    "print('Class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "<p>We have already decided our model to be the decision trees, so let's proceed to the training step. The <a href='http://scikit-learn.org/stable/'>Scikit-learn</a> package provides the off-the-shelf implementation of various machine learning models / algorithms, including the decision trees. We can simply use it to build a decision tree for our training set:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion : impurity function\n",
    "# max_depth : maximum depth of tree\n",
    "# random_state : seed of random number generator\n",
    "tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                              max_depth=3, \n",
    "                              random_state=0)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>NOTE: you are not required to standardize the data features before building a decision tree (or a random forest) because the information gain of a cutting point does not change when we scale values of an attribute.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "<p>Now we have a tree. Let's apply it to our testing set to see how it performs:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X_test)\n",
    "print('Misclassified samples: %d' % (y_test != y_pred).sum())\n",
    "print('Accuracy (tree): %.2f' % ((y_test == y_pred).sum() / y_test.shape[0]))\n",
    "\n",
    "# a more convenient way to evaluate a trained model is to use the sklearn.metrics \n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy (tree, sklearn): %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We get a 96% accuracy. That's pretty good!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "<p>Decision trees are an attractive model if we care about the interpretability of a model. By visualizing a tree, we can understand how a prediction is made by breaking down a classification rule into a series of questions about the data features.</p>\n",
    "<p>A nice feature of the <code>DecisionTreeClassifier</code> in Scikit-learn is that it allows us to export the decision tree as a <code>.dot</code> file after training, which we can visualize using the <a href='http://www.graphviz.org/'>GraphViz</a> program. This program is freely available on Linux, Windows, and Mac OS X. For exmaple, if you have <a href='https://www.anaconda.com/download/'>Anaconda</a> installed, you can get GraphViz by simply executing the following command in command line:</p>\n",
    "<p><code>> conda install graphviz</code></p>\n",
    "<p>After installing GraphViz, we can create the <code>tree.dot</code> file:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./output/\") : os.mkdir(\"./output/\")\n",
    "export_graphviz(\n",
    "    tree,\n",
    "    out_file='./output/tree.dot', \n",
    "    feature_names=X.columns.values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>and then convert the <code>tree.dot</code> file into a PNG file by executing the following GraphViz command from the command line under the same directory where <code>tree.dot</code> resides:</p>\n",
    "<p><code>> dot -Tpng tree.dot -o fig-tree.png</code></p>\n",
    "<p>Here is the visualized tree:</p>\n",
    "<p><img src='./output/fig-tree.png'></p>\n",
    "<p>As we can see, the criterion 'Flavanoids &lt;= 1.575' is effective in separating the data points of the first class from those of the third class. By looking into the other criteria, we also know how to separate data points of the second class from the rests.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random forests have gained huge popularity in applications of machine learning during the last decade due to their good classification performance, scalability, and ease of use. Intuitively, a random forest can be considered as an ensemble of decision trees. The idea behind **ensemble** learning is to combine weak learners to build a more robust model, a strong learner, that has a better generalization performance. The random forest algorithm can be summarized in four simple steps:\n",
    "\n",
    "1. Randomly draw $M$ **bootstrap samples** from the training set with replacement.\n",
    "2. Grow a decision tree from the bootstrap samples. At each node:\n",
    "    - **Randomly select** $K$ **features** without replacement.\n",
    "    - Split the node by finding the best cut among the selected features that maximizes the information gain.\n",
    "3. Repeat the steps 1 to 2 $T$ times to get $T$ trees.\n",
    "4. Aggregate the predictions made by different trees via the majority vote.\n",
    "\n",
    "Although random forests don't offer the same level of interpretability as decision trees, a big advantage of random forests is that we don't have to worry so much about the depth of trees since the majority vote can \"absorb\" the noise from individual trees. Therefore, we typically don't need to prune the trees in a random forest. The only parameter that we need to care about in practice is the number of trees $T$ at step 3. Generally, the larger the number of trees, the better the performance of the random forest classifier at the expense of an increased computational cost. Another advantage is that the computational cost can be distributed to multiple cores / machines since each tree can grow independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "<p>We can build a random forest by using the <code>RandomForestClassifier</code> in Scikit-learn:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion : impurity function\n",
    "# n_estimators :  number of decision trees\n",
    "# random_state : seed used by the random number generator\n",
    "# n_jobs : number of cores for parallelism\n",
    "forest = RandomForestClassifier(criterion='entropy',\n",
    "                                n_estimators=200, \n",
    "                                random_state=1,\n",
    "                                n_jobs=2)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = forest.predict(X_test)\n",
    "print('Accuracy (forest): %.2f' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We get a slightly improved accuracy 98%!</p>\n",
    "\n",
    "NOTE: in most implementations, including the `RandomForestClassifier` implementation in Scikit-learn, the bootstrap sample size $M$ is equal to the number of samples $N$ in the original training set by default. For the number of features $K$ to select at each split, the default that is used in Scikit-learn (and many other implementations) is  $K=\\sqrt{D}$ , where $D$ is the number of features of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Feature Importance\n",
    "<p>In addition to classification, a random forest can be used to calculate the <b>feature importance</b>. Using a random forest, we can measure feature importance as the averaged information gain (impurity decrease) computed from all decision trees in the forest.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "# get sort indices in descending order\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            X.columns.values[indices[f]], \n",
    "                            importances[indices[f]]))\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Feature Importances')\n",
    "plt.bar(range(X_train.shape[1]),\n",
    "        importances[indices],\n",
    "        align='center',\n",
    "        alpha=0.5)\n",
    "\n",
    "plt.xticks(range(X_train.shape[1]), \n",
    "           X.columns.values[indices], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/fig-forest-feature-importances.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure, we can see that \"Flavanoids\", \"OD280/OD315 of diluted wines\", \"Proline\", and \"Color intensity\" are the most important features to classify the Wine dataset. This may change if we choose a different number of trees $T$ in a random foreest. For example, if we set $T=10000$, then the most important feature becomes \"Color intensity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "By discarding the unimportant features, we can reduce the dimension of data points and compress data. For example, $Z_{Forest}$ is a compressed 2-D dataset that contains only the most important two features \"Flavanoids\" and \"OD280/OD315 of diluted wines:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_forest = X[['Flavanoids', 'OD280/OD315 of diluted wines']].values\n",
    "\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "for l, c, m in zip(np.unique(y.values), colors, markers):\n",
    "    plt.scatter(Z_forest[y.values==l, 0], \n",
    "                Z_forest[y.values==l, 1], \n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.title('Z_forest')\n",
    "plt.xlabel('Flavanoids')\n",
    "plt.ylabel('OD280/OD315 of diluted wines')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/fig-forest-z.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth mentioning that Scikit-learn also implements a class called <code>SelectFromModel</code> that helps you select features based on a user-specified threshold, which is useful if we want to use the <code>RandomForestClassifier</code> as a feature selector. For example, we could set the threshold to 0.16 to get $Z_{Forest}$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put prefitted RandomForestClassifier \"forest\" into SelectFromModel\n",
    "sfm = SelectFromModel(forest, threshold=0.16, prefit=True)\n",
    "Z_forest_alt = sfm.transform(X.values)\n",
    "\n",
    "for f in range(Z_forest_alt.shape[1]):\n",
    "    print(\"%2d) %-*s %f\" % (f + 1, 30, \n",
    "                            X.columns.values[indices[f]], \n",
    "                            importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension Reduction: PCA vs. Random Forest\n",
    "So far, we have seen two dimension reduction techniques: PCA and feature selection based on Random Forest. PCA is an <i>unsupervised</i> dimension reduction technique since it does not require the class labels; while the latter is a <i>supervised</i> dimension reduction technique as the labels are used for computing the information gain for each node split. However, PCA is a <b>feature extraction</b> technique (as opposed to <b>feature selection</b>) in the sense that a reduced feature may not be identical to any of the original features. Next, let's build classifiers for the two compressed datasets $Z_{PCA}$ and $Z_{Forest}$ and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train a decision tree based on Z_forest\n",
    "\n",
    "Z_forest_train, Z_forest_test, y_forest_train, y_forest_test = train_test_split(\n",
    "    Z_forest, y, test_size=0.3, random_state=0)\n",
    "\n",
    "tree_forest = DecisionTreeClassifier(criterion='entropy', \n",
    "                                     max_depth=3, \n",
    "                                     random_state=0)\n",
    "tree_forest.fit(Z_forest_train, y_forest_train)\n",
    "\n",
    "y_forest_pred = tree_forest.predict(Z_forest_test)\n",
    "print('Accuracy (tree_forest): %.2f' % accuracy_score(y_forest_test, y_forest_pred))\n",
    "\n",
    "# train a decision tree based on Z_pca\n",
    "\n",
    "# load Z_pca that we have created in our last lab\n",
    "Z_pca= np.load('./output/Z_pca.npy')\n",
    "# random_state should be the same as that used to split the Z_forest\n",
    "Z_pca_train, Z_pca_test, y_pca_train, y_pca_test = train_test_split(\n",
    "    Z_pca, y, test_size=0.3, random_state=0)\n",
    "\n",
    "tree_pca = DecisionTreeClassifier(criterion='entropy', \n",
    "                                  max_depth=3, \n",
    "                                  random_state=0)\n",
    "tree_pca.fit(Z_pca_train, y_pca_train)\n",
    "\n",
    "y_pca_pred = tree_pca.predict(Z_pca_test)\n",
    "print('Accuracy (tree_pca): %.2f' % accuracy_score(y_pca_test, y_pca_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As we can see, the tree grown from PCA-compressed data yields the same accuracy 96% as that of the tree for uncompressed data. Furthermore, it performs much better than the tree grown from the selected features advised by a random forest. This shows that PCA, a feature extraction technique, is effective in preserving \"information\" in a dataset when the compressed dimension is very low (2 in this case). The same holds for the Random Forest classifiers:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a random forest based on Z_forest\n",
    "\n",
    "forest_forest = RandomForestClassifier(criterion='entropy',\n",
    "                                n_estimators=200, \n",
    "                                random_state=1,\n",
    "                                n_jobs=2)\n",
    "forest_forest.fit(Z_forest_train, y_forest_train)\n",
    "\n",
    "y_forest_pred = forest_forest.predict(Z_forest_test)\n",
    "print('Accuracy (forest_forest): %.2f' % accuracy_score(y_forest_test, y_forest_pred))\n",
    "\n",
    "\n",
    "# train a random forest based on Z_pca\n",
    "\n",
    "forest_pca = RandomForestClassifier(criterion='entropy',\n",
    "                                n_estimators=200, \n",
    "                                random_state=1,\n",
    "                                n_jobs=2)\n",
    "forest_pca.fit(Z_pca_train, y_pca_train)\n",
    "\n",
    "y_pca_pred = forest_pca.predict(Z_pca_test)\n",
    "print('Accuracy (forest_pca): %.2f' % accuracy_score(y_pca_test, y_pca_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Visualization\n",
    "When the data dimension is 2, we can easily plot the decision boundaries of a classifier. Let's take a look at the decision boundaries of the Decision Tree and Random Forest classifiers we have for $Z_{PCA}$ and $Z_{Forest}$. First, we define a utility function for plotting decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[: len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(\n",
    "            x = X[y == cl, 0],\n",
    "            y = X[y == cl, 1],\n",
    "            alpha = 0.8,\n",
    "            c = [cmap(idx)], # Prevents warning\n",
    "            marker = markers[idx],\n",
    "            label = cl\n",
    "        )\n",
    "\n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        # plot all samples\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(X_test[:, 0],\n",
    "                    X_test[:, 1],\n",
    "                    c=[cmap(i - 1) for i in y_test],\n",
    "                    alpha=1.0,\n",
    "                    linewidths=1,\n",
    "                    marker='o',\n",
    "                    s=55, label='test set', edgecolors='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Next, we plot the decision boundaries by combining the training and testing sets deterministically:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boundaries of tree_forest\n",
    "Z_forest_combined = np.vstack((Z_forest_train, Z_forest_test))\n",
    "y_forest_combined = np.hstack((y_forest_train, y_forest_test))\n",
    "plot_decision_regions(\n",
    "        Z_forest_combined, \n",
    "        y_forest_combined, \n",
    "        classifier = tree_forest, \n",
    "        test_idx = range(\n",
    "            y_forest_train.shape[0],\n",
    "            y_forest_train.shape[0] + y_forest_test.shape[0]\n",
    "        )\n",
    ")\n",
    "\n",
    "plt.title('Tree_forest')\n",
    "plt.xlabel('Flavanoids')\n",
    "plt.ylabel('OD280/OD315 of diluted wines')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/fig-boundary-tree-forest.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# plot boundaries of tree_pca\n",
    "Z_pca_combined = np.vstack((Z_pca_train, Z_pca_test))\n",
    "y_pca_combined = np.hstack((y_pca_train, y_pca_test))\n",
    "plot_decision_regions(\n",
    "    Z_pca_combined, \n",
    "    y_pca_combined, \n",
    "    classifier = tree_pca, \n",
    "    test_idx = range(\n",
    "        y_pca_train.shape[0],\n",
    "        y_pca_train.shape[0] + y_pca_test.shape[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "plt.title('Tree_pca')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/fig-boundary-tree-pca.png', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the decision boundaries of a decision tree are always axis-aligned. This means that if a \"true\" boundary is not axis-aligned, the tree needs to be very deep to approximate the boundary using the \"staircase\" one. We can see this from the random forests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot boundaries of tree_forest\n",
    "plot_decision_regions(Z_forest_combined, \n",
    "                      y_forest_combined, \n",
    "                      classifier=forest_forest, \n",
    "                      test_idx=range(y_forest_train.shape[0],\n",
    "                                     y_forest_train.shape[0] + y_forest_test.shape[0]))\n",
    "\n",
    "plt.title('Forest_forest')\n",
    "plt.xlabel('Flavanoids')\n",
    "plt.ylabel('OD280/OD315 of diluted wines')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/fig-boundary-forest-forest.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# plot boundaries of tree_pca\n",
    "plot_decision_regions(Z_pca_combined, \n",
    "                      y_pca_combined, \n",
    "                      classifier=forest_pca, \n",
    "                      test_idx=range(y_pca_train.shape[0],\n",
    "                                     y_pca_train.shape[0] + y_pca_test.shape[0]))\n",
    "\n",
    "plt.title('Forest_pca')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/fig-boundary-forest-pca.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment</center>\n",
    "\n",
    "We try to make predition from another dataset <a href='https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)'>breast cancer wisconsin</a>. But there are too many features in this dataset. Please try to improve accuracy per feature $$\\frac{\\text{Accuracy}}{\\text{# Features}}$$\n",
    "\n",
    "HINT:\n",
    "- You can improve the ratio by picking out several important features.\n",
    "- The ratio can be improved from 0.03 up to 0.44."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\"\"\"\n",
    "\n",
    "# load the breast_cancer dataset\n",
    "init_data = load_breast_cancer()\n",
    "(X, y) = load_breast_cancer(return_X_y=True)\n",
    "# X = pd.DataFrame(data=X, columns=init_data['feature_names'])\n",
    "# y = pd.DataFrame(data=y, columns=['label'])\n",
    "\n",
    "# split X into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Train a RandomForestClassifier as model \n",
    "forest = RandomForestClassifier(criterion='entropy',\n",
    "                                n_estimators=200, \n",
    "                                random_state=1,\n",
    "                                n_jobs=2)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred = forest.predict(X_test)\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "print('Accuracy per feature: %.2f' % (accuracy_score(y_test, y_pred)/X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements:\n",
    "\n",
    "- Submit to **eeclass** with your code named ```Lab03_{student-id}.ipynb``` (e.g. ```Lab03_110069999.ipynb```).\n",
    "- **The code file should only contain the Assignment part.**\n",
    "- Remember to save the file after you rendered the output images in your notebook.\n",
    "- Deadline: **2024-01-07 (Sun) 23:59**."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "251px",
    "width": "254px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
