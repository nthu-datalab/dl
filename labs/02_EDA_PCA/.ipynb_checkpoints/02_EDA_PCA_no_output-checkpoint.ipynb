{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDaYaNxyCGzA"
   },
   "source": [
    "# <center> Exploring and Visualizing Data</center>\n",
    "<center>\n",
    "        Shan-Hung Wu & DataLab\n",
    "        <br>\n",
    "        Fall 2023\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANhFclYqCGzF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inline plotting instead of popping out\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np # numpy 1.26.0\n",
    "import pandas as pd # pandas 2.1.1\n",
    "import matplotlib.pyplot as plt # matplotlib 3.8.0\n",
    "import seaborn as sns # seaborn 0.13.0\n",
    "from sklearn.preprocessing import StandardScaler # scikit-learn 1.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU1KdMj3CGzI"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis (EDA) is an important and recommended first step of Machine Learning (prior to the training of a machine learning model that are more commonly seen in research papers). EDA performs the **exploration** and **exploitation** steps iteratively. In the exploration step, you \"explore\" the data, usually by visualizing them in different ways, to discover some characteristics of data. Then, in the exploitation step, you use the identified characteristics to figure out the next things to explore. You then repeat the above two steps until you are satisfied with what you have learned from the data.\n",
    "Data visualization plays an important role in EDA. Next, we use the [Wine](https://archive.ics.uci.edu/ml/datasets/Wine) dataset from the UCI machine learning repository as an example dataset and show some common and useful plots.\n",
    "\n",
    "## Visualizing the Important Characteristics of a Dataset\n",
    "\n",
    "Let's download the [Wine](https://archive.ics.uci.edu/ml/datasets/Wine) dataset using [Pandas](http://pandas.pydata.org/) first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzNikrXmCGzJ",
    "outputId": "f30f8684-aa08-4d07-f36a-af99f46b7ae8"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n",
    "    header = None)\n",
    "\n",
    "df.columns = [\n",
    "    'Class label', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash',\n",
    "    'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols',\n",
    "    'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',\n",
    "    'Proline'\n",
    "]\n",
    "\n",
    "X = df.drop(labels='Class label', axis=1)\n",
    "y = df['Class label']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQZrc1faCGzM"
   },
   "source": [
    "As we can see, showing data row-by-row with their column names does not help us get the \"big picture\" and characteristics of data.\n",
    "\n",
    "\n",
    "NOTE: `pd.read_csv()` function returns a [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) object. Pandas Dataframe is an useful \"two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes\".\n",
    "\n",
    "\n",
    "\n",
    "### Pairwise Joint Distributions\n",
    "We can instead see the joint distribution of any pair of columns/attributes/variables/features by using the pairplot function offered by [Seaborn](https://stanford.edu/~mwaskom/software/seaborn/), which is based on [Matplotlib](http://matplotlib.org/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpV51tpNCGzN",
    "outputId": "1e2f7a32-8863-4a3e-800b-fbaf9ceb02c4"
   },
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "sns.pairplot(df, vars=df.columns[1:], hue=\"Class label\", height=2.5, palette=sns.color_palette('deep', df['Class label'].unique().size))\n",
    "plt.tight_layout()\n",
    "\n",
    "if not os.path.exists('./output'):\n",
    "  os.makedirs('./output')\n",
    "plt.savefig('./output/fig-wine-scatter.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjSGJdtSCGzP"
   },
   "source": [
    "<p>From the above figures, we can easily see that there is a linear relationship between the \"Total phenols\" and \"Flavanoids\" variables. Furthermore, the class label cannot be easily predicted by a single variable. You may have noticed that the figures along the diagonal look different. They are histograms of values of individual variables. We can see that the \"Ash\" and \"Alcalinity of ash\" variables are roughly normally distributed.</p>\n",
    "\n",
    "<p>NOTE: importing the Seaborn library modifies the default aesthetics of Matplotlib for the current Python session. If you do not want to use Seaborn's style settings, you can reset the Matplotlib settings by calling:</p>\n",
    "\n",
    "```\n",
    ">>> sns.reset_orig()\n",
    "```\n",
    "### Correlation Matrix\n",
    "Showing the pairwise join distributions may still be overwhelming when we have a lot of variables in the dataset. Sometimes, we can just plot the correlation matrix to quantify the linear relationship between variables. The **correlation coefficient** between two random variables $a$ and $b$ are defined as:\n",
    "\n",
    "$$\\frac{\\mathrm{Cov}(\\mathrm{a},\\mathrm{b})}{\\sqrt{\\mathrm{Var}(\\mathrm{a})\\mathrm{Var}(\\mathrm{b})}}.$$\n",
    "\n",
    "Basically, it is the \"normalized\" variance that captures the **linear** relationship of the two random variables, and the values are bounded to the range $[âˆ’1,1]$. The correlation matrix $ \\boldsymbol{R}\\in\\mathbb{R}^{D\\times D}$ of a random vector $\\mathrm{x}$ is a square matrix whose each element $R_{i, j}$ denotes the correlation between the attributes $\\mathrm{x}_{i}$  and $\\mathrm{x}_{j}$ . If we regard data points as the i.i.d. samples of $\\mathrm{x}$ , then we can have an estimate $\\hat{\\boldsymbol{R}}$ whose each element\n",
    "\n",
    "$$\\hat{R}_{i,j}=\\frac{\\Sigma_{s=1}^{N}(x_{i}^{(s)}-\\hat{\\mu}_{\\mathrm{x}_{i}})(x_{j}^{(s)}-\\hat{\\mu}_{\\mathrm{x}_{j}})}{\\sqrt{\\Sigma_{s=1}^{N}(x_{i}^{(s)}-\\hat{\\mu}_{\\mathrm{x}_{i}})^{2}}\\sqrt{\\Sigma_{s=1}^{N}(x_{j}^{(s)}-\\hat{\\mu}_{\\mathrm{x}_{j}})^{2}}}=\\frac{\\hat{\\sigma}_{\\mathrm{x}_{i},\\mathrm{x}_{j}}}{\\hat{\\sigma}_{\\mathrm{x}_{i}}\\hat{\\sigma}_{\\mathrm{x}_{j}}}$$\n",
    "\n",
    "is an estimate of the correlation (usually called the Pearson's r) between attribute  $\\mathrm{x}_i$ and $\\mathrm{x}_j$ . Note that if we **z-normalize** each data point such that\n",
    "\n",
    "$$z_{i}^{(s)}=\\frac{x_{i}^{(s)}-\\hat{\\mu}_{\\mathrm{x}_{i}}}{\\hat{\\sigma}_{\\mathrm{x}_{i}}}$$\n",
    "\n",
    "for all $i$ . Then we simply have $\\hat{\\boldsymbol{R}}=\\frac{1}{N}\\boldsymbol{Z}^\\top \\boldsymbol{Z}$ , where $\\boldsymbol{Z}$ is the design matrix of the normalized data points. We can plot $\\hat{\\boldsymbol{R}}$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CEiP9DmICGzR",
    "outputId": "24d5081c-ef26-4f32-d04c-510d2f9afe67"
   },
   "outputs": [],
   "source": [
    "# Z-normalize data\n",
    "sc = StandardScaler()\n",
    "Z = sc.fit_transform(X)\n",
    "# Estimate the correlation matrix\n",
    "R = np.dot(Z.T, Z) / df.shape[0]\n",
    "\n",
    "sns.set(font_scale=1.0)\n",
    "\n",
    "ticklabels = [s for s in X.columns]\n",
    "\n",
    "hm = sns.heatmap(\n",
    "    R,\n",
    "    cbar=True,\n",
    "    square=True,\n",
    "    yticklabels=ticklabels,\n",
    "    xticklabels=ticklabels\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/fig-wine-corr.png', dpi = 300)\n",
    "plt.show()\n",
    "\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iilNM918CGzS"
   },
   "source": [
    "<p>The correlation matrix gives a more concise view of the relationship between variables. Some models, such the linear regression, assume that the explanatory variables are linearly correlated to the target variable. A heatmap of correlations can help us select variables supporting this assumption. For example, if we want to predict the \"Hue\" variable using the linear regression, we may pick the \"Malic acid\" and \"Flavanoids\" as the explanatory variables.</p>\n",
    "\n",
    "<p>NOTE: we could have simply used the NumPy function</p>\n",
    "\n",
    "```\n",
    "  >>>R = np.corrcoef(df.values.T)\n",
    "```\n",
    "\n",
    "to get the estimate $\\hat{\\boldsymbol{R}}$ of the correlation matrix. We calculate $\\hat{\\boldsymbol{R}}$ by ourselves using the normalized design matrix $\\boldsymbol{Z}$ because we will reuse $\\boldsymbol{Z}$ later.\n",
    "\n",
    "## PCA for Visualization\n",
    "\n",
    "PCA reduces the dimension of data points, and has been widely used across different machine learning tasks. One application of PCA is to help the visualization of high-dimensional data, as discussed next. \n",
    "\n",
    "Principal component analysis finds a projection matrix $\\boldsymbol{W}=[w^{(1)}, ..., w^{(k)}] \\in \\mathbb{R}^{D\\times K}$ ,where $w^{i}$ are orthonormal vectors, such that each attribute $z^{pca}_{j} = w^{(j)T}z$ has the maximum variance Var($z^{pca}_{j}$). <br>\n",
    "This problem can be reduced to solve\n",
    "\n",
    "$$arg \\max_{w^{i}\\in \\mathbb{R}^{D}} w^{(i)T}Z^TZw^{(i)} ,\\hspace{5mu} for \\hspace{5mu} i \\in [1,\\hspace{3mu}K] $$\n",
    "\n",
    "by Rayleigh's Quotient, the optimal $w^{(i)}$ is given by the eigenvector of $Z^TZ$ (or $\\hat{\\boldsymbol{R}}$) corresponding to the $i$<span/>th largest eigenvalue.\n",
    "\n",
    "Let's summarize PCA in a few simple steps:\n",
    "\n",
    "1. Standardize the $D$ -dimensional dataset $\\boldsymbol{X}$ , e.g., via the $z$ -normalization, and get $\\boldsymbol{Z}$ ;\n",
    "2. Estimate the covariance matrix $\\hat{\\boldsymbol{R}}$;\n",
    "3. Decompose $\\hat{\\boldsymbol{R}}$ into its eigenvectors and eigenvalues;\n",
    "4. Select $K$ eigenvectors that correspond to the $K$ largest eigenvalues, where $K$ is the dimensionality of the new feature subspace ($k < d$);\n",
    "5. Construct a projection matrix $\\boldsymbol{W}$ from the top-$K$ eigenvectors;\n",
    "6. Transform the $D$-dimensional input dataset $\\boldsymbol{Z}$ using the projection matrix $\\boldsymbol{W}$.\n",
    "\n",
    "\n",
    "\n",
    "### Eigendecomposition\n",
    "\n",
    "Since we already have $\\boldsymbol{Z}$ and $\\hat{\\boldsymbol{R}}$ from the above. We can begin from the step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-46KgjVCGzT",
    "outputId": "ed7c0f9d-4405-45cc-a172-2a678d4279a2"
   },
   "outputs": [],
   "source": [
    "eigen_vals, eigen_vecs = np.linalg.eigh(R)\n",
    "\n",
    "print('\\nEigenvalues: \\n%s' % eigen_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iZsAMfwCGzU"
   },
   "source": [
    "NOTE: there is an `np.linalg.eig()` function in NumPy that also eigendecomposes matrices. The difference is that `np.linalg.eigh()` is optimized for symmetric matrices whose eigenvalues are always real numbers. The numerically less stable `np.linalg.eig()` can decompose non-symmetric square matrices and returns complex eigenvalues.\n",
    "\n",
    "### Eigenvector Selection\n",
    "\n",
    "In step 4, we need to decide the value of $K$. We can plot the <b>variance explained ratio</b> of each eigenvalue:\n",
    "\n",
    "$$\\frac{\\vert\\lambda_{j}\\vert}{\\Sigma_{j=1}^{D}\\vert\\lambda_{j}\\vert}$$\n",
    "\n",
    "in the descending order to help us decide how many eigenvectors to keep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkkEwYONCGzU",
    "outputId": "af23e36d-deb8-4232-86dd-72624c2043c9"
   },
   "outputs": [],
   "source": [
    "tot = sum(np.abs(eigen_vals))\n",
    "var_exp = [(i / tot) for i in sorted(np.abs(eigen_vals), reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "plt.bar(range(1, eigen_vals.size + 1), var_exp, alpha=0.5, align='center',\n",
    "        label='Individual')\n",
    "plt.step(range(1, eigen_vals.size + 1), cum_var_exp, where='mid',\n",
    "         label='Cumulative')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/fig-pca-var-exp.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nThrBG-CGzV"
   },
   "source": [
    "<p>The resulting plot indicates that the first principal component alone accounts for 40 percent of the variance. Also, we can see that the first two principal components combined explain almost 60 percent of the variance in the data. Next, we collect the two eigenvectors that correspond to the two largest values to capture about 60 percent of the variance in this dataset. Note that we only chose two eigenvectors for the purpose of illustration, since we are going to plot the data via a two-dimensional scatter plot later in this subsection. In practice, the number of principal components can be determined by other reasons, such as the trade-off between computational efficiency and performance.</p>\n",
    "\n",
    "### Feature Transformation\n",
    "\n",
    "<p>Let's now proceed with the last three steps to project the standardized Wine dataset onto the new principal component axes. We start by sorting the eigenpairs by decreasing order of the eigenvalues:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pg7EAU9cCGzW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFkWUcPDCGzX"
   },
   "source": [
    "<p>Next, we pick the first two eigenvectors and form the project matrix $\\boldsymbol{W}$:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRudUNQNCGzX",
    "outputId": "671cc385-953f-4b46-8e38-b3163e364d64"
   },
   "outputs": [],
   "source": [
    "W = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "               eigen_pairs[1][1][:, np.newaxis]))\n",
    "print('Projection matrix W:\\n', W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ek9lOIBICGzY"
   },
   "source": [
    "Finally, we can obtain the compressed dataset by:\n",
    "$$\\boldsymbol{Z}^{\\text{PCA}}=\\boldsymbol{Z}\\boldsymbol{W}$$\n",
    "and visualize it using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XORF9WF5CGzY",
    "outputId": "51b2012c-c713-47f6-89a4-e74c8982c070",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_pca = Z.dot(W)\n",
    "\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "for l, c, m in zip(np.unique(y.values), colors, markers):\n",
    "    plt.scatter(Z_pca[y.values==l, 0], \n",
    "                Z_pca[y.values==l, 1], \n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.title('Z_pca')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('./output/fig-pca-z.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5g9WzsUWCGzZ"
   },
   "source": [
    "As we can see, the data is more spread along the $x$-axis corresponding to the first principal component than the $y$-axis (second principal component), which is consistent with the explained variance ratio plot that we created in the previous subsection. PCA may save us from examining a lot of pairwise distributions (as shown in the previous section) when the original data dimension $D$ is high. For example, if we see that the data points with different labels can be separated in the space of PCA, then we can simply choose a linear classifier to do the classification.\n",
    "\n",
    "Finally, let's save the compressed dataset for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fPKzdY9CGzZ"
   },
   "outputs": [],
   "source": [
    "np.save('./output/Z_pca.npy', Z_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0PWtiypCGzc"
   },
   "source": [
    "# <center>Assignment</center>\n",
    "\n",
    "<p>\n",
    "Here's a generated dataset, with 3 classes and 15 attributes. Your goal is to reduce data dimension to 2 and 3, and then plot 2-D and 3-D visualization on the compressed data, respectively.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fE5x5PKxCGzc",
    "outputId": "fdffb8b1-701e-4c5b-a94e-27dcf523e9c8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# import libs, load data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\"\"\"\n",
    "\n",
    "df_load = pd.read_csv('https://nthu-datalab.github.io/ml/labs/02_EDA_PCA/gen_dataset.csv')\n",
    "\n",
    "X_load = df_load.drop(labels='Class label', axis=1)\n",
    "Y_load = df_load['Class label']\n",
    "\n",
    "df_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ot7f1xwCGzd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Z-normalize data\n",
    "sc = StandardScaler()\n",
    "Z = sc.fit_transform(X_load)\n",
    "# Estimate the correlation matrix\n",
    "R = np.dot(Z.T, Z) / df_load.shape[0]\n",
    "\n",
    "# Calculate the eigen values, eigen vectors\n",
    "eigen_vals, eigen_vecs = np.linalg.eigh(R)\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs.sort(reverse=True)\n",
    "\n",
    "# Form the projection matrix\n",
    "W_2D = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "               eigen_pairs[1][1][:, np.newaxis]))\n",
    "\n",
    "# You should form a projection matrix which projects from raw-data dimension to 3 dimension here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xr0-6cVCGzd"
   },
   "source": [
    "You can see [here](https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html) for information about plotting 3D graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPvkkDa-CGzd",
    "outputId": "20c93896-d88c-45b1-bff4-6b2026dcf76a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid', context='notebook')\n",
    "\n",
    "# import Axes3D for plottin 3d scatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# cacculate z_pca(2d and 3d)\n",
    "Z_pca2 = Z.dot(W_2D)\n",
    "\n",
    "# plot settings\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "# plot 2D\n",
    "plt2 = fig.add_subplot(1,2,1)\n",
    "for l, c, m in zip(np.unique(Y_load), colors, markers):\n",
    "    plt2.scatter(Z_pca2[Y_load==l, 0], \n",
    "                Z_pca2[Y_load==l, 1], \n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.title('Z_pca 2D')\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "\n",
    "# plot 3D\n",
    "plt3 = fig.add_subplot(1,2,2, projection='3d')\n",
    "# you should plot a 3D scatter using plt3.scatter here (see Axes3D.scatter in matplotlib)\n",
    "\n",
    "if not os.path.exists('./output'):\n",
    "    os.makedirs('./output')\n",
    "plt.savefig('./output/fig-pca-2-3-z.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8HxXk1fCGze"
   },
   "source": [
    "From this assignment, you can see the different results between different numbers of principle components chosen.\n",
    "\n",
    "## Requirements:\n",
    "- Submit to **eeclass** with your code named ```Lab02_{student-id}.ipynb``` (e.g. ```Lab02_109069999.ipynb```).\n",
    "- **The code file should only contain the Assignment part.**\n",
    "- Remember to save the file after you rendered the output images in your notebook.\n",
    "- Deadline: **2024-01-07 (Sun) 23:59**."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "name": "02_EDA_PCA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
